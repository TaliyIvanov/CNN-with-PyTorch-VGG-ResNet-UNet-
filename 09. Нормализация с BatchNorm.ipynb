{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a33e0973-ecb6-4a5c-be31-c53d516aaff8",
   "metadata": {},
   "source": [
    "### Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a5123-0478-46ea-9230-67a8f29cd3d0",
   "metadata": {},
   "source": [
    "[Документация](https://pytorch.org/docs/stable/nn.html#normalization-layers)\n",
    "\n",
    "[Видео с объяснениями от Сергея Дубинина](https://www.youtube.com/watch?v=CSRu8byqxNs&list=PLBP4Q3FNSLK2rtGPBsK-aMAetYj-8yg_1&index=17&ab_channel=%D0%A1%D0%B5%D1%80%D0%B3%D0%B5%D0%B9%D0%94%D1%83%D0%B1%D0%B8%D0%BD%D0%B8%D0%BD)\n",
    "\n",
    "BatchNorm, или Batch Normalization, это метод нормализации, применяемый в нейронных сетях для улучшения скорости обучения и стабильности. Он работает путем нормализации активаций каждого слоя для каждого мини-батча, вычитая среднее значение и деля на стандартное отклонение.\n",
    "\n",
    "**Как работает BatchNorm:**\n",
    "\n",
    "1. **Вычисление статистики мини-батча:** Для каждого признака (канала) в мини-батче вычисляется среднее значение и стандартное отклонение.\n",
    "\n",
    "2. **Нормализация:**  Каждый признак в мини-батче нормализуется с использованием вычисленных статистики: `z = (x - mean) / std`, где `x` - исходное значение признака, `mean` - среднее значение по мини-батчу, `std` - стандартное отклонение по мини-батчу.  Это приводит данные к распределению с нулевым средним и единичной дисперсией.\n",
    "\n",
    "3. **Масштабирование и сдвиг:**  Нормализованные значения затем масштабируются и сдвигаются с использованием обучаемых параметров `gamma` (гамма) и `beta` (бета):  `y = gamma * z + beta`. Это позволяет сети восстановить представление, которое могло быть потеряно при нормализации.\n",
    "\n",
    "\n",
    "**Преимущества использования BatchNorm:**\n",
    "\n",
    "* **Ускорение обучения:** BatchNorm позволяет использовать более высокие скорости обучения, что сокращает время обучения.\n",
    "* **Снижение чувствительности к инициализации весов:** Модель становится менее чувствительной к начальным значениям весов.\n",
    "* **Регуляризация:** BatchNorm действует как регуляризатор, уменьшая переобучение, особенно в небольших сетях, и иногда позволяет уменьшить необходимость в Dropout.\n",
    "* **Стабилизация обучения:** BatchNorm помогает с проблемой исчезающего и взрывающегося градиента.\n",
    "* **Улучшение качества модели:**  BatchNorm может привести к повышению точности модели.\n",
    "\n",
    "**Нюансы:**\n",
    "- Батчнорм можно применять, как перед активацией. так и после, как лучше, никто не скажет, необходимо экспериментировать.\n",
    "- У слоя, к которому применяется нормализация, необходимо отключить смещение. Ошибки не будет, но лучше его убрать.\n",
    "\n",
    "**Минусы использования батчнорм**\n",
    "- BatchNorm() не стоит использовать вместе с Dropout()\n",
    "- BatchNorm() плохо работает при маленьком размере batch-a\n",
    "- BatchNorm() по разному работает при обучении и валидации, аналогично Dropout()\n",
    "\n",
    "**Где применяется BatchNorm:**\n",
    "\n",
    "BatchNorm обычно применяется после линейных слоев или сверточных слоев и перед функцией активации.  Однако, точное размещение может варьироваться в зависимости от архитектуры сети.\n",
    "\n",
    "\n",
    "**BatchNorm в PyTorch:**\n",
    "\n",
    "В PyTorch BatchNorm реализован в модулях `torch.nn.BatchNorm1d`, `torch.nn.BatchNorm2d` и `torch.nn.BatchNorm3d` для одномерных, двумерных и трехмерных данных соответственно.  Например, для двумерных данных (изображений) используется `torch.nn.BatchNorm2d`.  Важно правильно установить параметр `num_features`, который соответствует количеству каналов во входных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b69d1a4-b3e7-420b-947b-a3a3d6e419a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования nn.Dropout()\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(20),  # BatchNorm() принимает 20 на вход из 1ого лин слоя\n",
    "    nn.Linear(20, 10),\n",
    "    nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08803e2b-b8f3-446e-8555-35546b28b94c",
   "metadata": {},
   "source": [
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebf39c8d-2df5-4d0a-a1d6-342268abe5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import time  # для замера времени\n",
    "\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0a8f3-863f-4dc2-a0e4-84897b4b64d8",
   "metadata": {},
   "source": [
    "### Рассмотрим на примере задачи классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f7d27-1c64-45bd-a366-d916a15cc0fc",
   "metadata": {},
   "source": [
    "#### Подготовка данных для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61236d04-cb04-4995-ab47-8ee4b612e63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd83ce59-f02e-4805-9f62-6503a1935263",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Пробуем для 'cuda'\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.len_dataset = 0 # длина датасет\n",
    "        self.data_list = [] # список кортежей путей до файла и позиции в onehot векторе\n",
    "\n",
    "        # итерируемся по папке с основными файлами\n",
    "        for path_dir, dir_list, file_list in os.walk(path):\n",
    "            if path_dir == path:\n",
    "                self.classes = sorted(dir_list)\n",
    "                self.class_to_idx = {\n",
    "                    cls_name: i for i, cls_name in enumerate(self.classes)\n",
    "                    }\n",
    "                continue\n",
    "\n",
    "            cls = path_dir.split('/')[-1]\n",
    "\n",
    "            for name_file in file_list:\n",
    "                file_path = os.path.join(path_dir, name_file)\n",
    "                self.data_list.append((file_path, self.class_to_idx[cls]))\n",
    "\n",
    "            self.len_dataset += len(file_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path, target = self.data_list[index]\n",
    "        sample = Image.open(file_path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "            target = self.transform(target)\n",
    "\n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "646f38df-32bb-4c47-b3f0-647e52a0fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование для изображений\n",
    "transform = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=(0.5, ), std=(0.5, ))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1bf4a39-7754-4cc2-abbe-c4b12525b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание датасетов\n",
    "train_data = MNISTDataset('mnist/training', transform=transform)\n",
    "test_data = MNISTDataset('mnist/testing', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67644850-16fd-4df4-a3c8-8abee66284d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = random_split(train_data, [0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b793b11-61e9-4e4d-8824-7f1bb52a7410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание загрузчиков\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a7fe60-aebc-4773-91fe-9e185b4d67fa",
   "metadata": {},
   "source": [
    "#### Создание модели с nn.BatchNorm1d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e3f0a74-b50c-475e-85a7-be61e4745b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем наш класс с BatchNorm1d()\n",
    "class MyModelBN(nn.Module):\n",
    "    def __init__(self, input, output):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input, 256)\n",
    "        self.layer_2 = nn.Linear(256, output)\n",
    "        self.batchnorm = nn.BatchNorm1d(256)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.batchnorm(x)\n",
    "        out = self.layer_2(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d4e60be-958f-4880-8c09-5d8f9c7a3e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем наш класс с dropout() для сравнения времени\n",
    "class MyModelDO(nn.Module):\n",
    "    def __init__(self, input, output):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input, 256)\n",
    "        self.layer_2 = nn.Linear(256, output)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        out = self.layer_2(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "46bfca2f-d4b7-45e8-8865-9a6577f67e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализируем модель с nn.BatchNorm1d()\n",
    "model_with_BatchNorm = MyModelBN(784, 10).to(device)\n",
    "# инициализируем модель с dropout() \n",
    "model_with_Dropout = MyModelDO(784, 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8afa5bb3-5202-4cec-b22f-86cce61822e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверяем правильность построения модели\n",
    "input = torch.rand([16, 784], dtype=torch.float32).to(device)\n",
    "\n",
    "out = model_with_BatchNorm(input)\n",
    "out.shape    # (16,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0672473-e4d7-47cc-a479-d9427013a8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверяем правильность построения модели\n",
    "input = torch.rand([16, 784], dtype=torch.float32).to(device)\n",
    "\n",
    "out = model_with_Dropout(input)\n",
    "out.shape    # (16,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b26175-c604-4841-b518-da825dbacf96",
   "metadata": {},
   "source": [
    "### Тренеровка модели с Dropout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2cb8b4-f1f2-4d53-b2b5-a223ef0b3318",
   "metadata": {},
   "source": [
    "Добавил в цикл обучения расчет времени на каждую эпоху, чтобы оценить \"ускорение\" от применения нормализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e848e6d4-edd5-4cc6-8b5c-fe29d28e1c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выбираем функцию потерь и оптимизатор градиентного спуска\n",
    "loss_model = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model_with_Dropout.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "409a4396-4dd4-451d-8b2f-cb62360b628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "lr_list = []\n",
    "best_loss = None\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "06f889a1-7cf8-4572-ba9d-a46fa3e922df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], train_loss=0.4371, train_acc=0.8694, val_loss=0.2426, val_acc=0.9281\n",
      "Время на эпоху 1: 19.59 секунд.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], train_loss=0.2334, train_acc=0.9297, val_loss=0.1632, val_acc=0.9510\n",
      "На эпохе: 2, сохранена модель со значением функции потерь на валидаци: 0.1632\n",
      "\n",
      "Время на эпоху 2: 19.62 секунд.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], train_loss=0.1805, train_acc=0.9463, val_loss=0.1430, val_acc=0.9578\n",
      "На эпохе: 3, сохранена модель со значением функции потерь на валидаци: 0.1430\n",
      "\n",
      "Время на эпоху 3: 19.47 секунд.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], train_loss=0.1581, train_acc=0.9522, val_loss=0.1267, val_acc=0.9627\n",
      "На эпохе: 4, сохранена модель со значением функции потерь на валидаци: 0.1267\n",
      "\n",
      "Время на эпоху 4: 20.19 секунд.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], train_loss=0.1376, train_acc=0.9577, val_loss=0.1277, val_acc=0.9616\n",
      "Время на эпоху 5: 20.16 секунд.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], train_loss=0.1267, train_acc=0.9617, val_loss=0.1166, val_acc=0.9642\n",
      "На эпохе: 6, сохранена модель со значением функции потерь на валидаци: 0.1166\n",
      "\n",
      "Время на эпоху 6: 20.04 секунд.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], train_loss=0.1174, train_acc=0.9633, val_loss=0.1190, val_acc=0.9662\n",
      "Время на эпоху 7: 19.86 секунд.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], train_loss=0.1077, train_acc=0.9662, val_loss=0.0980, val_acc=0.9714\n",
      "На эпохе: 8, сохранена модель со значением функции потерь на валидаци: 0.0980\n",
      "\n",
      "Время на эпоху 8: 19.98 секунд.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], train_loss=0.1033, train_acc=0.9663, val_loss=0.1091, val_acc=0.9683\n",
      "Время на эпоху 9: 19.64 секунд.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], train_loss=0.0989, train_acc=0.9689, val_loss=0.0962, val_acc=0.9723\n",
      "На эпохе: 10, сохранена модель со значением функции потерь на валидаци: 0.0962\n",
      "\n",
      "Время на эпоху 10: 20.13 секунд.\n"
     ]
    }
   ],
   "source": [
    "# Цикл обучения\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    start_time = time.time()  # Засекаем время начала эпохи\n",
    "    \n",
    "    # Тренировка модели\n",
    "    model_with_Dropout.train()\n",
    "    running_train_loss = []\n",
    "    true_answer = 0\n",
    "    # добавим трейн луп, чтобы видеть прогресс обучения модели\n",
    "    train_loop = tqdm(train_loader, leave=False)\n",
    "    for x, targets in train_loop:\n",
    "        # Данные\n",
    "        # (batch.size, 1, 28, 28) --> (batch.size, 784)\n",
    "        x = x.reshape(-1, 28*28).to(device)\n",
    "        # (batch.size, int) --> (batch.size, 10), dtype=float32\n",
    "        targets = targets.reshape(-1).to(torch.int32)\n",
    "        targets = torch.eye(10)[targets].to(device)\n",
    "\n",
    "        # Прямой проход + расчет ошибки модели\n",
    "        pred = model_with_Dropout(x)\n",
    "        loss = loss_model(pred, targets)\n",
    "\n",
    "        # Обратный проход\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Шаг оптимизации\n",
    "        opt.step()\n",
    "\n",
    "        running_train_loss.append(loss.item())\n",
    "        mean_train_loss = sum(running_train_loss)/len(running_train_loss)\n",
    "\n",
    "        true_answer += (pred.argmax(dim=1) == targets.argmax(dim=1)).sum().item()\n",
    "\n",
    "        train_loop.set_description(f\"Epoch [{epoch+1}/{EPOCHS}], train_loss={mean_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # Расчет значения метрики\n",
    "    running_train_acc = true_answer / len(train_data)\n",
    "\n",
    "    # Сохранение значения функции потерь и метрики\n",
    "    train_loss.append(mean_train_loss)\n",
    "    train_acc.append(running_train_acc)\n",
    "\n",
    "    # Проверка модели (Валидация)\n",
    "    model_with_Dropout.eval()\n",
    "    with torch.no_grad():\n",
    "        running_val_loss = []\n",
    "        true_answer = 0\n",
    "        for x, targets in val_loader:\n",
    "            # Данные\n",
    "            # (batch.size, 1, 28, 28) --> (batch.size, 784)\n",
    "            x = x.reshape(-1, 28*28).to(device)\n",
    "            # (batch.size, int) --> (batch.size, 10), dtype=float32\n",
    "            targets = targets.reshape(-1).to(torch.int32)\n",
    "            targets = torch.eye(10)[targets].to(device)\n",
    "\n",
    "            # Прямой проход + расчет ошибки модели\n",
    "            pred = model_with_Dropout(x)\n",
    "            loss = loss_model(pred, targets)\n",
    "\n",
    "            running_val_loss.append(loss.item())\n",
    "            mean_val_loss = sum(running_val_loss)/len(running_val_loss)\n",
    "\n",
    "            true_answer += (pred.argmax(dim=1) == targets.argmax(dim=1)).sum().item()\n",
    "\n",
    "        # Расчет значения метрики\n",
    "        running_val_acc = true_answer / len(val_data)\n",
    "\n",
    "        # Сохранение значения функции потерь и метрики\n",
    "        val_loss.append(mean_val_loss)\n",
    "        val_acc.append(running_val_acc)\n",
    "\n",
    "        lr_scheduler.step(mean_val_loss)\n",
    "        lr = lr_scheduler._last_lr[0]\n",
    "        lr_list.append(lr)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], train_loss={mean_train_loss:.4f}, train_acc={running_train_acc:.4f}, val_loss={mean_val_loss:.4f}, val_acc={running_val_acc:.4f}\")\n",
    "\n",
    "        # добавляем две проверки, для сохранения лучшей модели\n",
    "        if best_loss is None:\n",
    "            best_loss = mean_val_loss\n",
    "      \n",
    "        if mean_val_loss < best_loss:\n",
    "            best_loss = mean_val_loss\n",
    "            \n",
    "            # если модель улучшила свои показатели, то отсчет эпох пойдет заново\n",
    "            # обнуляем счетчик\n",
    "            count = 0\n",
    "            \n",
    "            # так же сохраняем словарь в случае улучшения модели\n",
    "            checkpoint = {\n",
    "                'state_model': model_with_Dropout.state_dict(),\n",
    "                'state_opt': opt.state_dict(),\n",
    "                'state_lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'loss':{\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                    'best_loss': best_loss\n",
    "                },\n",
    "                'metric':{\n",
    "                    'train_acc': train_acc,\n",
    "                    'val_acc': val_acc\n",
    "                },\n",
    "                'lr': lr_list,\n",
    "                'epoch':{\n",
    "                    'EPOCHS': EPOCHS,\n",
    "                    'save_epoch': epoch\n",
    "                }\n",
    "            }\n",
    "    \n",
    "            \n",
    "    \n",
    "            torch.save(checkpoint, f'model_state_dict_epoch_{epoch+1}.pt')\n",
    "            print(f\"На эпохе: {epoch+1}, сохранена модель со значением функции потерь на валидаци: {mean_val_loss:.4f}\", end='\\n\\n')\n",
    "\n",
    "        # условие, для остановки обучения по достижению счетчиком определенного значения!\n",
    "        if count >= 10:\n",
    "            print(f'\\033[31mОбучение остановлено на {epoch + 1} эпохе.\\033[0m')\n",
    "            break\n",
    "            \n",
    "        # в конце каждой эпохи увеличиваем счетчик на 1\n",
    "        count += 1\n",
    "\n",
    "    # Засекаем время конца эпохи\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Время на эпоху {epoch + 1}: {epoch_time:.2f} секунд.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3b040e-534a-47d2-a568-9752b75290cd",
   "metadata": {},
   "source": [
    "### Тренеровка модели с BatchNorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f0afa200-9048-47f2-a2e8-13910476610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выбираем функцию потерь и оптимизатор градиентного спуска\n",
    "loss_model = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model_with_BatchNorm.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9e1592b2-a14b-4342-a0af-6d8cef6ff27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "lr_list = []\n",
    "best_loss = None\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "202f8cb9-97cd-4329-82ce-8ae99850a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], train_loss=0.3847, train_acc=0.8898, val_loss=0.3352, val_acc=0.9040\n",
      "Время на эпоху 1: 19.95 секунд.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], train_loss=0.2990, train_acc=0.9135, val_loss=0.2694, val_acc=0.9211\n",
      "На эпохе: 2, сохранена модель со значением функции потерь на валидаци: 0.2694\n",
      "\n",
      "Время на эпоху 2: 19.58 секунд.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], train_loss=0.2697, train_acc=0.9231, val_loss=0.2518, val_acc=0.9272\n",
      "На эпохе: 3, сохранена модель со значением функции потерь на валидаци: 0.2518\n",
      "\n",
      "Время на эпоху 3: 19.81 секунд.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], train_loss=0.2549, train_acc=0.9253, val_loss=0.2466, val_acc=0.9287\n",
      "На эпохе: 4, сохранена модель со значением функции потерь на валидаци: 0.2466\n",
      "\n",
      "Время на эпоху 4: 20.31 секунд.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], train_loss=0.2383, train_acc=0.9309, val_loss=0.2475, val_acc=0.9291\n",
      "Время на эпоху 5: 19.66 секунд.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], train_loss=0.2303, train_acc=0.9316, val_loss=0.2469, val_acc=0.9233\n",
      "Время на эпоху 6: 19.53 секунд.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], train_loss=0.2192, train_acc=0.9349, val_loss=0.2357, val_acc=0.9302\n",
      "На эпохе: 7, сохранена модель со значением функции потерь на валидаци: 0.2357\n",
      "\n",
      "Время на эпоху 7: 20.09 секунд.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], train_loss=0.2130, train_acc=0.9369, val_loss=0.2321, val_acc=0.9315\n",
      "На эпохе: 8, сохранена модель со значением функции потерь на валидаци: 0.2321\n",
      "\n",
      "Время на эпоху 8: 20.14 секунд.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], train_loss=0.2067, train_acc=0.9387, val_loss=0.2240, val_acc=0.9344\n",
      "На эпохе: 9, сохранена модель со значением функции потерь на валидаци: 0.2240\n",
      "\n",
      "Время на эпоху 9: 20.09 секунд.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], train_loss=0.1996, train_acc=0.9400, val_loss=0.2274, val_acc=0.9366\n",
      "Время на эпоху 10: 20.07 секунд.\n"
     ]
    }
   ],
   "source": [
    "# Цикл обучения\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    start_time = time.time()  # Засекаем время начала эпохи\n",
    "    \n",
    "    # Тренировка модели\n",
    "    model_with_BatchNorm.train()\n",
    "    running_train_loss = []\n",
    "    true_answer = 0\n",
    "    # добавим трейн луп, чтобы видеть прогресс обучения модели\n",
    "    train_loop = tqdm(train_loader, leave=False)\n",
    "    for x, targets in train_loop:\n",
    "        # Данные\n",
    "        # (batch.size, 1, 28, 28) --> (batch.size, 784)\n",
    "        x = x.reshape(-1, 28*28).to(device)\n",
    "        # (batch.size, int) --> (batch.size, 10), dtype=float32\n",
    "        targets = targets.reshape(-1).to(torch.int32)\n",
    "        targets = torch.eye(10)[targets].to(device)\n",
    "\n",
    "        # Прямой проход + расчет ошибки модели\n",
    "        pred = model_with_BatchNorm(x)\n",
    "        loss = loss_model(pred, targets)\n",
    "\n",
    "        # Обратный проход\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Шаг оптимизации\n",
    "        opt.step()\n",
    "\n",
    "        running_train_loss.append(loss.item())\n",
    "        mean_train_loss = sum(running_train_loss)/len(running_train_loss)\n",
    "\n",
    "        true_answer += (pred.argmax(dim=1) == targets.argmax(dim=1)).sum().item()\n",
    "\n",
    "        train_loop.set_description(f\"Epoch [{epoch+1}/{EPOCHS}], train_loss={mean_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # Расчет значения метрики\n",
    "    running_train_acc = true_answer / len(train_data)\n",
    "\n",
    "    # Сохранение значения функции потерь и метрики\n",
    "    train_loss.append(mean_train_loss)\n",
    "    train_acc.append(running_train_acc)\n",
    "\n",
    "    # Проверка модели (Валидация)\n",
    "    model_with_BatchNorm.eval()\n",
    "    with torch.no_grad():\n",
    "        running_val_loss = []\n",
    "        true_answer = 0\n",
    "        for x, targets in val_loader:\n",
    "            # Данные\n",
    "            # (batch.size, 1, 28, 28) --> (batch.size, 784)\n",
    "            x = x.reshape(-1, 28*28).to(device)\n",
    "            # (batch.size, int) --> (batch.size, 10), dtype=float32\n",
    "            targets = targets.reshape(-1).to(torch.int32)\n",
    "            targets = torch.eye(10)[targets].to(device)\n",
    "\n",
    "            # Прямой проход + расчет ошибки модели\n",
    "            pred = model_with_BatchNorm(x)\n",
    "            loss = loss_model(pred, targets)\n",
    "\n",
    "            running_val_loss.append(loss.item())\n",
    "            mean_val_loss = sum(running_val_loss)/len(running_val_loss)\n",
    "\n",
    "            true_answer += (pred.argmax(dim=1) == targets.argmax(dim=1)).sum().item()\n",
    "\n",
    "        # Расчет значения метрики\n",
    "        running_val_acc = true_answer / len(val_data)\n",
    "\n",
    "        # Сохранение значения функции потерь и метрики\n",
    "        val_loss.append(mean_val_loss)\n",
    "        val_acc.append(running_val_acc)\n",
    "\n",
    "        lr_scheduler.step(mean_val_loss)\n",
    "        lr = lr_scheduler._last_lr[0]\n",
    "        lr_list.append(lr)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], train_loss={mean_train_loss:.4f}, train_acc={running_train_acc:.4f}, val_loss={mean_val_loss:.4f}, val_acc={running_val_acc:.4f}\")\n",
    "\n",
    "        # добавляем две проверки, для сохранения лучшей модели\n",
    "        if best_loss is None:\n",
    "            best_loss = mean_val_loss\n",
    "      \n",
    "        if mean_val_loss < best_loss:\n",
    "            best_loss = mean_val_loss\n",
    "            \n",
    "            # если модель улучшила свои показатели, то отсчет эпох пойдет заново\n",
    "            # обнуляем счетчик\n",
    "            count = 0\n",
    "            \n",
    "            # так же сохраняем словарь в случае улучшения модели\n",
    "            checkpoint = {\n",
    "                'state_model': model_with_BatchNorm.state_dict(),\n",
    "                'state_opt': opt.state_dict(),\n",
    "                'state_lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'loss':{\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                    'best_loss': best_loss\n",
    "                },\n",
    "                'metric':{\n",
    "                    'train_acc': train_acc,\n",
    "                    'val_acc': val_acc\n",
    "                },\n",
    "                'lr': lr_list,\n",
    "                'epoch':{\n",
    "                    'EPOCHS': EPOCHS,\n",
    "                    'save_epoch': epoch\n",
    "                }\n",
    "            }\n",
    "    \n",
    "            \n",
    "    \n",
    "            torch.save(checkpoint, f'model_state_dict_epoch_{epoch+1}.pt')\n",
    "            print(f\"На эпохе: {epoch+1}, сохранена модель со значением функции потерь на валидаци: {mean_val_loss:.4f}\", end='\\n\\n')\n",
    "\n",
    "        # условие, для остановки обучения по достижению счетчиком определенного значения!\n",
    "        if count >= 10:\n",
    "            print(f'\\033[31mОбучение остановлено на {epoch + 1} эпохе.\\033[0m')\n",
    "            break\n",
    "            \n",
    "        # в конце каждой эпохи увеличиваем счетчик на 1\n",
    "        count += 1\n",
    "\n",
    "    # Засекаем время конца эпохи\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Время на эпоху {epoch + 1}: {epoch_time:.2f} секунд.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e965cb-84bc-48b6-943a-7d5f61849ea1",
   "metadata": {},
   "source": [
    "### Выводы сравнения скорости работы с BatchNorm() и DropOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f8d6d1-bbb6-4849-9cd9-4ecf8cb8a213",
   "metadata": {},
   "source": [
    "Ну в целом, не сказать, что модель с BatchNorm тренеруется быстрее модели с DropOut, вероятно, данное преимущество раскрывается на существенно более Больших моделях и других данных.\n",
    "\n",
    "Так же, возможно, что некорректно замерять скорость именно эпохи, возможно стоит замерять скорость выполнения именно тренировочной части эпохи... Еще до валидации\n",
    "\n",
    "Чтож, будем эксперементировать в будущем =)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
